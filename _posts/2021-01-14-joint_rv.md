---
layout: posts
title:  "(1) Pairs of Random Variables"
categories: prob
author_profile: false
use_math: true
sidebar:
    nav: "docs"
---


##  Pairs of Discrete Random Variables
Let B = {$X$ in $A_1$} $\cap$ {$Y$ in $A_2$}
then, an equation below is formed.

$$
\begin{align*}
    P[B] &= P[\{X \ in \ A_1\} \cap \{Y \ in \ A_2 \}] \triangleq P[X \ in \ A_1, Y \ in \ A_n]
\end{align*}
$$

## Joint Probability Mass Function(pmf)
Let the vector random variable $\mathbf{X} = (X, Y)$ assume the values from some *countable* set $S_{X, Y} \ = \ \{(x_j, y_k), j \ = \ 1, \ 2, \ldots, \ k \ = \ 1, \ 2, \ldots}$, then **the joint probability mass function** of **X** specifies the probabilities of the event $$\{X \ in \ x\} \cap \{Y \ in \ y\} : p_{x, y}  \triangleq P[x \ = \ x, \ Y \ = \ y]$$

###### $\bullet$ PMF on the set $S_{X, Y}$
$$p_{x, y}(x, y) \triangleq P[X \ = \ x_j, \ Y \ = \ y_k] \ (x_k. y_j) \qquad\subset S_{X, Y}$$

###### $\bullet$ The probability of any event B
$$
    P[\mathbf{X} \ in \ B] = \sum_{(x_j, y_k) \ in \ B}\sum_{in \mathbf{B}} p_{(X, Y)}(x_j, \ y_k)
$$

###### $\bullet$ When B is the entire space of B...
$$
    \sum_{j=1}^{\infty}\sum_{k=1}^{\infty}P_{(X, Y)}(x_j, y_k) = 1
$$
###### Marginal Probability Mass Function
$$
\begin{align*}
    p_x(x_j) &= P[X \ = \ x_j] \\
             &= P[X \ = \ x_j, Y \ = \ anything] \\
             &= \sum_{k=1}^{\infty}p_{X, Y}(x_j, \ y_k)
\end{align*}
$$

## The Joint CDF of X and Y
The **joint cumulative distribution functio of X and Y(joint CDF)** is defined as theh probability of the event $$ \{ X \leq x_1 \} \cap \{Y \leq y_1 \}$$ : 
$$
    F_{X, Y} \ = \ P[x \ \leq \ x_1, \ Y \ \leq \ y_1]
$$

###### Properties of the joint CDF
1. The joint cdf is a nondecreasing function of x and y
   $F_{X, Y}(x_1, \ x_2) \ \leq F_{X, Y}(x_2 \ , \ y_2) \qquad if \ \ x_1 \leq x_2 \ and \ y_1 \leq y_2 $
2. $F_{X, \ Y}(x_1, \ -\infty) \ = \ 0, \qquad  F_{X, Y}(-\infty, \ y_1) \ = \ 0$
3. Marginal CDF
   
   $$F_X(x_1) = F_{X, Y}(x_1, \ \infty) \ and \ F_y(y_1) = F_{X, Y}(\infty, y_1)$$
4. The joint cdf's continuous condition
   $$
        \lim_{x \rightarrow a^{\dagger}} F_{X, Y}(x, y) \ = \ F_{X, Y}(a, y) \ and \ \lim_{y \rightarrow b^{\dagger}}F_{X, Y}(x, y) \ = \ F_{X, Y}(x, \ b) 
   $$

   
## The joint PDF function
###### Probability Density Function
$$ P[\mathbf{X} \ in \ B] \ = \ \int_B \int f_{X, Y}(x^{'}, y^{'})d x^{'}d y^{'} $$

###### Sum of the Total Events
$$ 1 = \int_{\infty}^{\infty} \int_{\infty}^{\infty}f_{X, Y}(x^{'}, y^{'})d x^{'}d y^{'} $$

###### Relationship between CDF and PDF
$$
    \begin{align*}
        F_{X, Y}(x, y) &= \int_{-\infty}^{x}\int_{-\infty}^{y}f_{X, Y}(x^{'}, y^{'})d x^{'} d y^{'} \\
        f_{X, Y} &= \dfrac{\partial^2 F_{X, Y}(x, y)}{\partial x \partial y}
    \end{align*}
$$

###### The probability of a rectangular region
$$
    \begin{align*}
        B &= \{ (x, y) : a_1 \ < x\ \leq b_1 \ and\ a_2\ < y \leq \ b_2 \} \\
        P[B] &= \int_{a_1}^{b_1}\int_{a_2}^{b_2}f_{X, Y}(\dot{x}, \dot{y})d\dot{x}d\dot{y}
    \end{align*}
$$

###### Marginal PDF
$$
    \begin{align*}
        f_x(x) &= \dfrac{d}{dx}\int_{\infty}^{x}  \int_{\infty}^{\infty}f_{X, Y}(\dot{x}, \dot{y})d\dot{y} \} d\dot{y} \\
               &= \int_{\infty}^{\infty}f_{X, Y}(x,\ \dot{y})d\dot{y}
    \end{align*}
$$

## Independence of Two Random Variables
###### Independent Random Variables
X and Y are independent random variable if and only if the formula below is true.

If X and Y are *continuous random variables...*

$$
    \begin{align*}
        P[X \ in \  A_1, \ Y \ in \ A_2] \ &= \ P[X \ in \ A_1] \ P[Y \ in \ A_2]
    \end{align*}
$$

If X and Y are *discrete random variables...*

$$
    \begin{align*}
        p_[X, Y] &= P[X = x_j\ , Y = y_k] \\
                 &= p_X(x_j)p_Y(y_k)
    \end{align*}
$$

## Joint Moments and Expected Values of a Function of Two Random Variables
- *Expectation* : The center of mass of the distribution of **X**
- *Variance* : The variance is defined as the expected value of $(X\ - \ m)^2$ which provides a measure of the *spread* of the distribution.

###### Expected Value of a Function of Two Random Variables
Let value $\mathbf{Z}\ = \ g(X, Y)$, then 

$$  E[z] \ = \
    \begin{cases}
        \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x, \ y)f_{X, \ Y}(x, \ y)dxdy & \quad X, Y \ jointly \ continuous \\
        \sum_i \sum_n g(x_i, \ y_n)p_{X, Y}(x_i, \ y_n) & \quad X, \ Y \ discrete.
    \end{cases}
$$

## Joint Moments, Correlation, and Covariance

###### The $(j, \ k)^{th}$ joint moment of X and Y
$$
    E[X^j Y^k] = 
    \begin{cases}
        \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x^j y^k f_{X, Y}(x, \ y) dx dy & \qquad X, \ Y, jointly continous \\
        \sum_i \sum_n x_i^j y_n^k p_{X, Y}(x_i, \ y_n) & \qquad X, \ Y, \ discrete
    \end{cases}
$$
###### Central Moment of X and Y

$$
    E[(X \ - \ E[x])^j(Y \ - \ E[Y])^k]
$$

###### Covariance of X and Y
At *central moment of X and Y*, if j=1, k=1, that is the *covariance of X and Y*

$$
    COV(X, \ Y) =  E[(X \ - \ E[x])(Y \ - \ E[Y])]
$$

$COV(X, \ Y)$ can be expands to the formula below

$$
    \begin{align*}
        COV(X, \ Y) &= E[XY \ - \ XE[Y] \ - \ YE[X] \ + \ E[X]E[Y]] \\
                    &= E[XY] \ - \ E[X]E[Y]
    \end{align*}
$$

$COV(X, \ Y)$ is zero if X and Y are independent.   $(E[XY] \ = \ E[X]E[Y])$


###### Correlation Coefficient
In order to know the relative value that tells how two random variables are correlated, we defines *correlation coefficinet*

Let $\sigma_x$ and $\sigma_y$ are the stadard deviations of X, Y, then the *correlation coefficient* is defined by


$$
    \rho_{X, Y} \ = \ \dfrac{COV(X, \ Y)}{\sigma_x \sigma_y} \ = \ \dfrac{E[XY] \ - \ E[X]E[Y]}{\sigma_x \sigma_y}
$$

The *correlation coefficient* is a number that is at most 1 magnitude

$$
    -1 \leq \rho_{X, Y} \leq 1
$$

therefore, it is able to be used as the relative factor.

## Conditional Probability and Conditional Expectation
###### Conditional Probability
The definition of the conditional probability is following below.

$$
    P[Y \ in \ A \ | \ X \ = \ x ] \ = \ \dfrac{P[Y \ in \ A, \ X \ = \ x]}{P[X \ = \ x]} \qquad for \ P[X \ = \ x] \ > \ 0
$$

###### Conditional PMF
$$
    p_Y(y | x) = P[Y \ = \ y \ | \ X \ = x] \ = \ \dfrac{P[X \ = \ x, \ Y \ = \ y]}{P[X \ = \ x]} \ = \ \dfrac{p_{X, Y}(x, \ y)}{p_X(x)}    \qquad for \ p_X(x) \ > 0
$$


###### The Properties of the Conditional PMF
- $$ p_Y(y_j | x_k) \ = \ \dfrac{P[X \ = \ x_k, \ Y \ = \ y_j]}{P[X \ = \ x_k]} \ = \ P[Y \ = \ y_j] \ = \ p_Y(y_j) $$
- $$p_{X, Y}(x_k, \ y_j) \ = \ p_Y(y_j \ x_k)p_X(x_k) \ \ and \ \ p_{X, Y}(x_k, \ y_j) \ = \ p_X(x_k | y_j)p_Y(y_j)$$
- $$P_[Y \ in \ A] \ = \ \sum_{all \ x_k}P[Y \ in \ A \ | \ X \ = \ x_k]p_X(x_k)$$

The conditional CDF is the same principle as the PMF.

## Pairs of Jointly Gaussian Random Variables
The jointly *The random variables X and Y are said to be jointly Gaussian if their joint pdf has the form*

$$
    f_{X, Y}(x, \ y) = \dfrac{\exp{\dfrac{-1}{2(1 \ - \ \rho_{x, y}^2)}[(\dfrac{x \ - \ m_1}{\rho_1})^2 \ - \ 2\rho_{x, y}(\dfrac{x \ - \ m_1}{\rho_1})(\dfrac{y \ - \ m_2}{\rho_2}) \ + \ (\dfrac{y \ - \ m_2}{\rho})^2]}}{2 \pi \rho_1 \rho_2 \sqrt{1 \ - \ \rho_{X,Y}^2}}
$$

The covariance of joint Gaussian is given below

$$
    COV(X, \ Y) \ = \ \rho_{X, Y}\sigma_1\sigma_2
$$

*proof)*

$$
    \begin{align*}
        COV(X, \ Y) &= E[(X \ - \ m_1)(Y \ - \ m_2)] \\
                    &= E[E[(X \ - \ m_1)(Y \ - \ m_2)] \ | \  Y] 
    \end{align*}
$$

The conditional expectation of given  $Y \ = \ y \$ is

$$
    \begin{align*}
        E[(X \ - \ m_1)(Y \ - \ m_2) \ | \ Y \ = \ y] &= (y \ - \ m_2)E[X \ - \ m_1 | Y \ = y] \\
                                                      &= (y \ - \ m_2)(E[X \ | \ Y \ = y] \ - \ m_1) \\
                                                      &= (y \ - \ m_2)(\rho_{X, Y}\dfrac{\sigma_1}{\sigma_2}(y \ - \ m2))
    \end{align*}
$$
therefore,
$$
    E[(X \ - \ m_1)(Y \ - \ m_2) \ | \ Y] \ = \ \rho_{X, Y} \dfrac{\rho_1}{\rho_2}E[(Y \ - \ m_2)^2]
$$

as a result, 

$COV(X, \ Y) = \rho_{X, Y}\sigma_1\sigma_2$



# Reference

Garcia, *Probability, Statistics, and Random Processes for Electrical Engineering* PEARSON(2009) p245-299