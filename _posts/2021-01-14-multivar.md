---
layout: posts
title:  "(2) Vector Random Variables"
categories: prob
author_profile: false
use_math: true
sidebar:
    nav: "docs"
---

## Vector Random Variables
Let **vector random variable X** be a function that assigns a vector of real numbers to each outcome $\xi$ in S, the sample space of the random experiment. Then **X** can be represent as a vector :
  
$$
    \begin{aligned}
        \mathbf{X} &= \ \begin{Bmatrix}
            X_1 \\
            X_2 \\
            \vdots \\
            X_n           
        \end{Bmatrix}        
    \end{aligned}
$$

## Events and Probabilities
For the n-dimensional random variable **X** = $(X_1, \ X_2, \ \ldots \ X_n)$, the product form of **X** is

$$
    \begin{aligned}
        A  \ &= \ \{ X_1 \ in \ A_1 \} \ \cup \ \{X_2 \ in \ A_2 \} \ \cup \ \ldots \ \cup \ \{ X_n \ in \ A_n \}
    \end{aligned}
$$

then, the probability of product form of X that we intrested :
$$
    \begin{aligned}
        P[A] &= \ P[\{ X_1 \ in \ A_1 \} \ \cup \ldots \cup \{ X_n \ in \ A_n \}] \\
             &\triangleq \ P[X_1 \ in \ A_1, \ X_2 \ in \ A_2, \ \ldots \, \ X_n \ in \ A_n]
    \end{aligned}
$$

## Joint Distribution Functions
The **joint cumulative distribution function(CDF)** of $X_1, \ X_2, \ \ldots, \ X_n$ is represented below.

$$
    \begin{align*}
        F_X(\mathbf{x}) \ \triangleq \ F_{X_1, \ X_2, \ \ldots, \ X_n}(x_1, \ x_2, \ \ldots, \ x_n) &= \ P[X_1 \ \leq \ x_1, \ X_2 \leq \ x_2, \ \ldots, \ X_n \ \leq \ x_n]
    \end{align*}
$$

The **joint probability mass function** of n discrete random varibales is defined by

$$
    \begin{aligned}
        P[\mathbf{X} \ in \ A] \ &\triangleq \ p_{X_1, \ X_2, \ \ldots, \ X_n}(x_1, \ x_2, \ldots, \ x_n) \\
                                 &= P[X_1 \ = \ x_1, \ X_2 \ = \ x_2, \ldots, \ X_n \ = \ x_n] 
    \end{aligned}
$$

##### The probability including the certain event *A*
###### Case 1 : Continuous Random Variable
$$
    \begin{aligned}
        P[X \ in \ \mathbf{A}] \ &= \ \int_{X \ in \ A} \ \ldots \ \int f_{X_1, \ X_2, \ \ldots, \ X_n}(\dot{x_1}, \ \dot{x_2, \ \ldots, \ \dot{x_n}})d\dot{x_1}d\dot{x_2} \ \ldots \ d\dot{x_n}
    \end{aligned}
$$
###### Case 2 : Discrete Random Variable

$$
    \begin{aligned}
        P[\mathbf{X} \ in \ A] \ &= \ \sum_{X \ in \ A}  \ldots \ \sum \ p_{X_1, \ X_2 \ \ldots , X_n}(x_1, \ x_2, \ \ldots, \ x_n) 
    \end{aligned}
$$

## Marginal PMF and PDF


##### Marginal PMF
Let **X** be a discrete n-dimensional joint random variable, then the marginal pmf $X_k$ is

$$
    \begin{aligned}
        p_{X_k}(x_k) \ &= \ \sum_{x_1} \ \sum_{x_2} \ \ldots \sum_{x_n} \ p(\mathbf{x}) \\
    \end{aligned}
$$
##### Marginal PDF
Let **X** be a continuous n-dimensional joint random variable, then the marginal pdf $X_k$ is

$$
    \begin{aligned}
        f_{x_k}(x_k) \ &= \int_{-\infty}^{\infty} \ \ldots \ \int_{-\infty}^{\infty} \ f_{\mathbf{X}}(\mathbf{x})dx_1 dx_2 \ \ldots \ dx_n
    \end{aligned}
$$

## Condional Probability
The pdf of $X_n$ given the values of $X_1, \ \ldots, X_n$ is given by
$$
    \begin{aligned}
        f_{X_n}(x_n \ | \ x_1, \ldots, \ x_{n-1})  \ &= \ \dfrac{f_{X_1, \ \ldots, \ X_{n}(x_1, \ \ldots, \ x_n)}}{f_{X_1, \ \ldots, \ X_{n-1}(x_1, \ \ldots, \ x_{n-1})}} 
    \end{aligned}
$$

The pmf of $X_n$ given the values of $X_1, \ \ldots, X_n$ is given by
$$
    \begin{aligned}
        p_{X_n}(x_n \ | \ x_1, \ldots, \ x_{n-1})  \ &= \ \dfrac{p_{X_1, \ \ldots, \ X_{n}(x_1, \ \ldots, \ x_n)}}{p_{X_1, \ \ldots, \ X_{n-1}(x_1, \ \ldots, \ x_{n-1})}} 
    \end{aligned}
$$

## General Transformation of PDF
![trans](/assets/img_prob/prob1.png)

Let
$$
    V \ = \ g_1(X, \ Y) \ \ and \ \ W \ = \ g_2(X, \ Y)
$$

Assume the functions $h_1$ and $h_2$ are respectively inverse function of $g_1$ and $g_2$, then
$$
    x \ = \ h_1(v, \ w) \ and \ y \ = \ h_2(v, \ w)
$$

Obeserving the figure above, the linear approximation can be used and it yields

$$
    g_k(x \ + \ dx, \ y) \ \backsimeq \ g_k(x, \ y) \ + \ \dfrac{\partial}{\partial x}g_k(x, \ y)dx \quad k \ = \ 1, 2 
$$

Because of the formula above, figure(b) can be represented.

Therefore, the relationship of *pdf*s of X, Y and V, W is
$$
    \begin{aligned}
        f_{X, \ Y}(x, \ y)dxdy \ &= \ g_{V, \ W}(v, \ w)dp \\
        f_{V, \ W}(v, \ w) \ &= \ \dfrac{f_{X, \ Y}(h_1(v, \ w), h_2(v, \ w))}{\Bigg\vert \dfrac{dp}{dxdy} \Bigg\vert}
    \end{aligned}
$$

Because of dP is the area of the parallelogram,

$$ 
    \begin{aligned}
        \rvert dp \lvert \ &= \  det\begin{bmatrix}
        \dfrac{\partial g_1}{\partial x}dx & \dfrac{\partial g_1}{\partial y}dy \\
        \dfrac{\partial g_2}{\partial x}dx & \dfrac{\partial g_2}{\partial y}dy
    \end{bmatrix} 
    \end{aligned}
$$

As a result,
$$ 
    \begin{aligned}
        \Bigg\vert \dfrac{dP}{dxdy} \Bigg\vert \ &= \  det\begin{bmatrix}
        \dfrac{\partial g_1}{\partial x} & \dfrac{\partial g_1}{\partial y} \\
        \dfrac{\partial g_2}{\partial x} & \dfrac{\partial g_2}{\partial y}
    \end{bmatrix} 
    \end{aligned}
$$

Let $\Bigg\vert\dfrac{dP}{dxdy}\Bigg\vert$ be $J(x, \ y)$, then $J(v, \ w)J(x, \ y) \ = \ 1$
$$
    \begin{aligned}
        f_{V, \ W}(v, \ w) \ &= \ \dfrac{f_{X, \ Y}(h_1(v, \ w), h_2(v, \ w))}{\vert J(x, \ y) \vert} \\
                             &= \ f_{X, \ Y}(h_1(v, \ w), h_2(v, \ w))\vert \ J(v, \ w)\vert
    \end{aligned}
$$

# Expected Values of Vector Random Variables

##### Expectated Value
Let X be a n-dimensional vector random variables, then expected value of a function g(**X**) is given by :

$$
E[Z] \ = \
    \begin{cases}
        \int_{-\infty}^{\infty} \ldots \int_{-\infty}^{\infty}g(x_1, \ x_2, \ \ldots, \ x_n)f_{\mathbf{x}}(x_1, \ x_2, \ \ldots, \ x_n)dx_1dx_2 \ldots dx_n \qquad for \ continous \ \mathbf{X} \\
        \sum_{x_1}\ldots \sum_{x_n}g(x_1, \ x_2, \ \ldots \, x_n)p_{\mathbf{x}}(x_1, \ x_2, \ \ldots, \ x_n) \qquad \qquad for \ discrete \ \mathbf{X}

    \end{cases}
$$

##### Linearity of *Expectation* Operation
$$
    \begin{aligned}
        E[\sum_k g_k(\mathbf{X})] \ &= \ \sum_k E[g_k(\mathbf{X})]
    \end{aligned}
$$

##### Independency at Expectation
Let $X_1, \ X_2, \ \ldots \ X_n \ \in \ \mathbf{X}$ be independent, then
$$
    \begin{aligned}
        E[\prod_k g_k(\mathbf{X})] \ &= \ \prod_k E[g_k(\mathbf{X})]
    \end{aligned}
$$

# Mean Vector and Covariance Matrix

##### Mean Vector

For $\mathbf{X} \ = \ (X_1, \ X_2, \ \ldots, \ X_n)$, the **meanvector** is defined as the *column vector* of expected values of the componnets $X_k$ :
$$
    \mathbf{m_x} \ = \ E[\mathbf{X}] \ = \ E \begin{bmatrix}
        X_1 \\ X_2 \\ \vdots \\ X_n
    \end{bmatrix}
    \ \triangleq \ \begin{bmatrix}
        E[X_1] \\ E[X_2] \\ \vdots \\ E[X_n]
    \end{bmatrix}
$$

##### Correlation Matrix
The correlation matrix, the second moments of **X** is defined :
$$
    \mathbf{R_x} \ \triangleq \ 
    \begin{bmatrix}
        E[X_1^2]    &   E[X_1X_2] & \ldots & E[X_1 X_n] \\
        E[X_2 X_1]  &   E[X_2^2]  & \ldots & E[X_2 X_n] \\
        \vdots      & \vdots      & \ddots & \vdots \\
        E[X_n X_1] & E{X_n X_2}   & \ldots & E[X_n^2] 
    \end{bmatrix} 
$$

###### Covarian Matrix
The **covariance matrix** has the second-order central moments as its entries :
$$
    \mathbf{K_X} \ = \ \begin{bmatrix}
        E[(X_1 \ - \ m_1)^2]                & E[(X_1 \ - \ m_1)(X_2 \ - \ m_2)] & \ldots & E[(X_1 \ - \ m_1)(X_n \ - \ m_n)] \\
        E[(X_2 \ - \ m_2)(X_1 \ - \ m_2)]   & E[(X_2 \ - \ m_2)^2]              & \ldots & E[(X_2 \ - \ m_2)(X_n \ - \ m_n)] \\
        \vdots                              & \vdots                            & \ddots & \vdots                               \\
        E[(X_n \ - \ m_n)(X_1 \ - \ m_1)]   & E[(X_n \ - \ m_n)(X_2 \ - \ m_2)] & \ldots & E[(X_n \ - \ m_n)^2] \\
    \end{bmatrix}
$$

###### The relationship of $R_X$ and $K_X$
- If the all elements of **X** are uncorrelated each other(i.e, $COV(X_i, \ X_j) \ = \ 0 \qquad if \ i \ \neq \ j$)
  - $\mathbf{K_X}$ is a *diagonal matrix*
- If the all elements of **X** are indepedent each other
  - $\mathbf{K_X}$ is a *diagonal matrix*
- If $\mathbf{m_X}$ is a zero vector
  - $\mathbf{K_X} \ = \ \mathbf{R_X}$

Let *X* be n-dimensional random vector variables
$$
    \mathbf{X}\mathbf{X}^T \ = \ \begin{bmatrix}
        X_1^2   & X_1 X_2   &   \ldots  & X_1 X_n \\
        X_2 X_1 & X_2^2     &   \ldots  & X_2 X_n   \\
        \vdots  & \vdots    &   \ddots  & \vdots    \\
        X_n X_1 & X_n X_2   &   \ldots  & X_n^2
    \end{bmatrix}
$$
By the formula above,
$$
    \mathbf{R_X} \ = \ E[\mathbf{X}\mathbf{X}^T]
$$
and the covariance matrix is
$$
    \begin{aligned}
        \mathbf{K_X} \ &=   \ E[(\mathbf{X} \ - \ \mathbf{m_X})(\mathbf{X} \ - \ \mathbf{m_X})^T] \\
                        &=  \mathbf{R_X} \ - \ \mathbf{m_X}\mathbf{m_X}^T
    \end{aligned}
$$

##### Linear Transformation of Random Vectors

Let Y and X be n-dimensional vector random variable and $Y \ = \ AX$, then
$$
    \mathbf{Y} \ = \ \begin{bmatrix}
        a_{11} & a_{12} & \ldots & a_{1n} \\
        a_{21} & a_{22} & \ldots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \ldots & a_{nn}
    \end{bmatrix}
    \ = \ \mathbf{AX}
$$

The expected value of the $k^{th}$ row $Y_k$ is then,
$$
    E[Y_k] \ = \ E[\sum_{j}a_{kj}X_j] \ = \ \sum_{j}a_{kj}E[X_j]
$$
**Y** can be represented below :
$$
    \mathbf{m_y} \ = \ \begin{bmatrix}
        \sum_j a_{1j}E[x_j] \\ \sum_j a_{2j}E[x_j] \\ \vdots \\ \sum_j a_{nj}E[x_j]  
    \end{bmatrix}
    \ = \ \mathbf{A}E[\mathbf{X}] \ = \ A\mathbf{m_X}
$$

###### Covariance of Y and Cross-covariance
The covariance matrix of **Y** is then :
$$
    \begin{aligned}
        \mathbf{K_Y} \ &= \ E[(\mathbf{Y} \ - \ \mathbf{m_Y})(\mathbf{Y} \ - \ \mathbf{m_Y})^T] \\
                       &= \ E[(\mathbf{AX} \ - \ \mathbf{A m_X})(\mathbf{AX} \ - \ \mathbf{AX} \ - \ \mathbf{Am_X})] \\
                       &= \ E[\mathbf{A}(\mathbf{X} \ - \ \mathbf{m_X})(\mathbf{X} \ - \ mathbfm_X)\mathbf{A}^T] \\
                       &= \ \mathbf{A}E[(\mathbf{X} \ - \ \mathbf{m_X})(\mathbf{X} \ - \ \mathbf{m_X})^T]\mathbf{A}^T \\
                       &= \mathbf{AK_X}\mathbf{A}^T
    \end{aligned}
$$

The **cross-covariance** matrix of two random vectors **X** and **Y** is definedf as :
$$
    \begin{aligned}
    \mathbf{K_{XY}} \ &= \ E[(\mathbf{X} \ - \ \mathbf{m_X})(\mathbf{Y} \ - \ \mathbf{m_Y})^T] \\
                      &= \ E[\mathbf{X}\mathbf{Y}^T] \ - \ \mathbf{m_X}\mathbf{m_Y}^T           \\
                      &= \ \mathbf{R_{XY}} \ - \ \mathbf{m_X}\mathbf{m_Y}^T  \\
                      &= \ \mathbf{K_X}\mathbf{A}^T
    \end{aligned}
$$

Reference : Garcia, *Probability, Statistics, and Random Processes for Electrical Engineering* PEARSON(2009) p315-359