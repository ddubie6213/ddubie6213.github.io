---
layout: posts
title:  "(2) Vector Random Variables"
categories: prob
author_profile: false
use_math: true
sidebar:
    nav: "docs"
---

## Vector Random Variables
Let **vector random variable X** be a function that assigns a vector of real numbers to each outcome $\xi$ in S, the sample space of the random experiment. Then **X** can be represent as a vector :
  
$$
    \begin{aligned}
        \mathbf{X} &= \ \begin{Bmatrix}
            X_1 \\
            X_2 \\
            \vdots \\
            X_n           
        \end{Bmatrix}        
    \end{aligned}
$$

## Events and Probabilities
For the n-dimensional random variable **X** = $(X_1, \ X_2, \ \ldots \ X_n)$, the product form of **X** is

$$
    \begin{aligned}
        A  \ &= \ \{ X_1 \ in \ A_1 \} \ \cup \ \{X_2 \ in \ A_2 \} \ \cup \ \ldots \ \cup \ \{ X_n \ in \ A_n \}
    \end{aligned}
$$

then, the probability of product form of X that we intrested :
$$
    \begin{aligned}
        P[A] &= \ P[\{ X_1 \ in \ A_1 \} \ \cup \ldots \cup \{ X_n \ in \ A_n \}] \\
             &\triangleq \ P[X_1 \ in \ A_1, \ X_2 \ in \ A_2, \ \ldots \, \ X_n \ in \ A_n]
    \end{aligned}
$$

## Joint Distribution Functions
The **joint cumulative distribution function(CDF)** of $X_1, \ X_2, \ \ldots, \ X_n$ is represented below.

$$
    \begin{align*}
        F_X(\mathbf{x}) \ \triangleq \ F_{X_1, \ X_2, \ \ldots, \ X_n}(x_1, \ x_2, \ \ldots, \ x_n) &= \ P[X_1 \ \leq \ x_1, \ X_2 \leq \ x_2, \ \ldots, \ X_n \ \leq \ x_n]
    \end{align*}
$$

The **joint probability mass function** of n discrete random varibales is defined by

$$
    \begin{aligned}
        P[\mathbf{X} \ in \ A] \ &\triangleq \ p_{X_1, \ X_2, \ \ldots, \ X_n}(x_1, \ x_2, \ldots, \ x_n) \\
                                 &= P[X_1 \ = \ x_1, \ X_2 \ = \ x_2, \ldots, \ X_n \ = \ x_n] 
    \end{aligned}
$$

##### The probability including the certain event *A*
###### Case 1 : Continuous Random Variable
$$
    \begin{aligned}
        P[X \ in \ \mathbf{A}] \ &= \ \int_{X \ in \ A} \ \ldots \ \int f_{X_1, \ X_2, \ \ldots, \ X_n}(\dot{x_1}, \ \dot{x_2, \ \ldots, \ \dot{x_n}})d\dot{x_1}d\dot{x_2} \ \ldots \ d\dot{x_n}
    \end{aligned}
$$
###### Case 2 : Discrete Random Variable

$$
    \begin{aligned}
        P[\mathbf{X} \ in \ A] \ &= \ \sum_{X \ in \ A}  \ldots \ \sum \ p_{X_1, \ X_2 \ \ldots , X_n}(x_1, \ x_2, \ \ldots, \ x_n) 
    \end{aligned}
$$

## Marginal PMF and PDF


##### Marginal PMF
Let **X** be a discrete n-dimensional joint random variable, then the marginal pmf $X_k$ is

$$
    \begin{aligned}
        p_{X_k}(x_k) \ &= \ \sum_{x_1} \ \sum_{x_2} \ \ldots \sum_{x_n} \ p(\mathbf{x}) \\
    \end{aligned}
$$
##### Marginal PDF
Let **X** be a continuous n-dimensional joint random variable, then the marginal pdf $X_k$ is

$$
    \begin{aligned}
        f_{x_k}(x_k) \ &= \int_{-\infty}^{\infty} \ \ldots \ \int_{-\infty}^{\infty} \ f_{\mathbf{X}}(\mathbf{x})dx_1 dx_2 \ \ldots \ dx_n
    \end{aligned}
$$

## Condional Probability
The pdf of $X_n$ given the values of $X_1, \ \ldots, X_n$ is given by
$$
    \begin{aligned}
        f_{X_n}(x_n \ | \ x_1, \ldots, \ x_{n-1})  \ &= \ \dfrac{f_{X_1, \ \ldots, \ X_{n}(x_1, \ \ldots, \ x_n)}}{f_{X_1, \ \ldots, \ X_{n-1}(x_1, \ \ldots, \ x_{n-1})}} 
    \end{aligned}
$$

The pmf of $X_n$ given the values of $X_1, \ \ldots, X_n$ is given by
$$
    \begin{aligned}
        p_{X_n}(x_n \ | \ x_1, \ldots, \ x_{n-1})  \ &= \ \dfrac{p_{X_1, \ \ldots, \ X_{n}(x_1, \ \ldots, \ x_n)}}{p_{X_1, \ \ldots, \ X_{n-1}(x_1, \ \ldots, \ x_{n-1})}} 
    \end{aligned}
$$

## General Transformation of PDF
![trans](/assets/img_prob/prob1.png)

Let
$$
    V \ = \ g_1(X, \ Y) \ \ and \ \ W \ = \ g_2(X, \ Y)
$$

Assume the functions $h_1$ and $h_2$ are respectively inverse function of $g_1$ and $g_2$, then
$$
    x \ = \ h_1(v, \ w) \ and \ y \ = \ h_2(v, \ w)
$$

Obeserving the figure above, the linear approximation can be used and it yields

$$
    g_k(x \ + \ dx, \ y) \ \backsimeq \ g_k(x, \ y) \ + \ \dfrac{\partial}{\partial x}g_k(x, \ y)dx \quad k \ = \ 1, 2 
$$

Because of the formula above, figure(b) can be represented.

Therefore, the relationship of *pdf*s of X, Y and V, W is
$$
    \begin{aligned}
        f_{X, \ Y}(x, \ y)dxdy \ &= \ g_{V, \ W}(v, \ w)dp \\
        f_{V, \ W}(v, \ w) \ &= \ \dfrac{f_{X, \ Y}(h_1(v, \ w), h_2(v, \ w))}{\Bigg\vert \dfrac{dp}{dxdy} \Bigg\vert}
    \end{aligned}
$$

Because of dP is the area of the parallelogram,

$$ 
    \begin{aligned}
        \rvert dp \lvert \ &= \  det\begin{bmatrix}
        \dfrac{\partial g_1}{\partial x}dx & \dfrac{\partial g_1}{\partial y}dy \\
        \dfrac{\partial g_2}{\partial x}dx & \dfrac{\partial g_2}{\partial y}dy
    \end{bmatrix} 
    \end{aligned}
$$

As a result,
$$ 
    \begin{aligned}
        \Bigg\vert \dfrac{dP}{dxdy} \Bigg\vert \ &= \  det\begin{bmatrix}
        \dfrac{\partial g_1}{\partial x} & \dfrac{\partial g_1}{\partial y} \\
        \dfrac{\partial g_2}{\partial x} & \dfrac{\partial g_2}{\partial y}
    \end{bmatrix} 
    \end{aligned}
$$

Let $\Bigg\vert\dfrac{dP}{dxdy}$ be $J(x, \ y)\Bigg\vert$, then $J(v, \ w)J(x, \ y) \ = \ 1$
$$
    \begin{aligned}
        f_{V, \ W}(v, \ w) \ &= \ \dfrac{f_{X, \ Y}(h_1(v, \ w), h_2(v, \ w))}{\vert J(x, \ y) \vert} \\
                             &= \ f_{X, \ Y}(h_1(v, \ w), h_2(v, \ w))\vert \ J(v, \ w)\vert
    \end{aligned}
$$
